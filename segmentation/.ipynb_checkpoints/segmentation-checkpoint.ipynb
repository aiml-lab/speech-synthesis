{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "num_features = 13\n",
    "speak_feats = 16 #features in speaker embedding\n",
    "num_conv_filters = 64\n",
    "num_conv_layers = 4 #layers in convolutional network excluding zeroth layer\n",
    "num_rnn_layers = 3\n",
    "dropout = 0.85\n",
    "num_hidden_gru = 1024\n",
    "beam_width= 20\n",
    "\n",
    "#Phonemes\n",
    "silence = \"SIL\"\n",
    "\n",
    "phonemes = [silence,'AA','AE','AH','AO','AW','AY','B','CH','D','DH','EH',\n",
    "            'ER','EY','F','G','HH','IH','IY','JH','K','L','M','N','NG','OW','OY',\n",
    "            'P','R','S','SH','T','TH','UH','UW','V','W','Y','Z','ZH']\n",
    "phoneme_dict = dict()\n",
    "for i in range(len(phonemes)):\n",
    "    phoneme_dict[phonemes[i]] = i\n",
    "\n",
    "pair_dict = []\n",
    "\n",
    "for i in range(len(phonemes)):\n",
    "    for j in range(i+1, len(phonemes)):\n",
    "        pair_dict.append((phonemes[i], phonemes[j]))\n",
    "\n",
    "num_phonemes = len(phonemes) \n",
    "num_labels = num_phonemes**2\n",
    "ctc_classes = num_labels + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Phonemes should be paired before encoding, basically goal of segmentation model is map phoneme to start and end point in speech\n",
    "\n",
    "def Linear(in_features, out_features, dropout=0):\n",
    "    \"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features)\n",
    "    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "\n",
    "def Embedding(num_embeddings, embedding_dim, padding_idx, std=0.01):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    m.weight.data.normal_(0, std)\n",
    "    return m\n",
    "\n",
    "def expand_speaker_embed(inputs_btc, speaker_embed=None, tdim=1):\n",
    "    ss = speaker_embed.size()\n",
    "    speaker_embed_btc = speaker_embed.unsqueeze(1).expand(\n",
    "        ss[0], inputs_btc.size(tdim), ss[-1])\n",
    "    return speaker_embed_btc\n",
    "\n",
    "def Conv1d(in_channels, out_channels, kernel_size, dropout=0, std_mul=4.0, **kwargs):\n",
    "    m = Conv1d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "    std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n",
    "    m.weight.data.normal_(mean=0, std=std)\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "class Conv1dGLU(nn.Module):\n",
    "    \"\"\"(Dilated) Conv1d + Gated linear unit + (optionally) speaker embedding\"\"\"\n",
    "\n",
    "    def __init__(self, n_speakers, speaker_embed_dim, phonemes,\n",
    "                 in_channels, out_channels, kernel_size,\n",
    "                 dropout, padding=None, dilation=1, causal=False, residual=False,\n",
    "                 *args, **kwargs):\n",
    "        super(Conv1dGLU, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        if padding is None:\n",
    "            # no future time stamps available\n",
    "            if causal:\n",
    "                padding = (kernel_size - 1) * dilation\n",
    "            else:\n",
    "                padding = (kernel_size - 1) // 2 * dilation\n",
    "        self.causal = causal\n",
    "\n",
    "        self.conv = Conv1d(in_channels, 2 * out_channels, kernel_size,\n",
    "                           dropout=dropout, padding=padding, dilation=dilation,\n",
    "                           *args, **kwargs)\n",
    "        if n_speakers > 1:\n",
    "            self.speaker_proj = Linear(speaker_embed_dim, out_channels)\n",
    "        else:\n",
    "            self.speaker_proj = None\n",
    "\n",
    "    def forward(self, x, speaker_embed=None):\n",
    "        return self._forward(x, speaker_embed, False)\n",
    "\n",
    "    def incremental_forward(self, x, speaker_embed=None):\n",
    "        return self._forward(x, speaker_embed, True)\n",
    "\n",
    "    def _forward(self, x, speaker_embed, is_incremental):\n",
    "        residual = x\n",
    "        np.array()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if is_incremental:\n",
    "            splitdim = -1\n",
    "            x = self.conv.incremental_forward(x)\n",
    "        else:\n",
    "            splitdim = 1\n",
    "            x = self.conv(x)\n",
    "            # remove future time steps\n",
    "            x = x[:, :, :residual.size(-1)] if self.causal else x\n",
    "\n",
    "        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "        if self.speaker_proj is not None:\n",
    "            softsign = F.softsign(self.speaker_proj(speaker_embed))\n",
    "            # Since conv layer assumes BCT, we need to transpose\n",
    "            softsign = softsign if is_incremental else softsign.transpose(1, 2)\n",
    "            a = a + softsign\n",
    "        x = a * torch.sigmoid(b)\n",
    "        return (x + residual) * math.sqrt(0.5) if self.residual else x\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.conv.clear_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayConv1d(nn.Module):\n",
    "    \"\"\"Weight normzlized Conv1d + Highway network (support incremental forward)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, padding=None,\n",
    "                 dilation=1, causal=False, dropout=0, std_mul=None, glu=False):\n",
    "        super(HighwayConv1d, self).__init__()\n",
    "        if std_mul is None:\n",
    "            std_mul = 4.0 if glu else 1.0\n",
    "        if padding is None:\n",
    "            # no future time stamps available\n",
    "            if causal:\n",
    "                padding = (kernel_size - 1) * dilation\n",
    "            else:\n",
    "                padding = (kernel_size - 1) // 2 * dilation\n",
    "        self.causal = causal\n",
    "        self.dropout = dropout\n",
    "        self.glu = glu\n",
    "\n",
    "        self.conv = Conv1d(in_channels, 2 * out_channels,\n",
    "                           kernel_size=kernel_size, padding=padding,\n",
    "                           dilation=dilation, dropout=dropout,\n",
    "                           std_mul=std_mul)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(x, False)\n",
    "\n",
    "    def incremental_forward(self, x):\n",
    "        return self._forward(x, True)\n",
    "\n",
    "    def _forward(self, x, is_incremental):\n",
    "        \"\"\"Forward\n",
    "        Args:\n",
    "            x: (B, in_channels, T)\n",
    "        returns:\n",
    "            (B, out_channels, T)\n",
    "        \"\"\"\n",
    "\n",
    "        residual = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if is_incremental:\n",
    "            splitdim = -1\n",
    "            x = self.conv.incremental_forward(x)\n",
    "        else:\n",
    "            splitdim = 1\n",
    "            x = self.conv(x)\n",
    "            # remove future time steps\n",
    "            x = x[:, :, :residual.size(-1)] if self.causal else x\n",
    "\n",
    "        if self.glu:\n",
    "            x = F.glu(x, dim=splitdim)\n",
    "            return (x + residual) * math.sqrt(0.5)\n",
    "        else:\n",
    "            a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "            T = torch.sigmoid(b)\n",
    "            return (T * a + (1 - T) * residual)\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.conv.clear_buffer()\n",
    "\n",
    "\n",
    "def get_mask_from_lengths(memory, memory_lengths):\n",
    "    \"\"\"Get mask tensor from list of length\n",
    "    Args:\n",
    "        memory: (batch, max_time, dim)\n",
    "        memory_lengths: array like\n",
    "    \"\"\"\n",
    "    mask = memory.data.new(memory.size(0), memory.size(1)).byte().zero_()\n",
    "    for idx, l in enumerate(memory_lengths):\n",
    "        mask[idx][:l] = 1\n",
    "    return ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Forward method to be used in model class, Map variables accordingly \n",
    "def forward(self, x, speaker_embed=None):\n",
    "        assert self.n_speakers == 1 or speaker_embed is not None\n",
    "\n",
    "        # expand speaker embedding for all time steps\n",
    "        speaker_embed_btc = expand_speaker_embed(x, speaker_embed)\n",
    "        if speaker_embed_btc is not None:\n",
    "            speaker_embed_btc = F.dropout(speaker_embed_btc, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Generic case: B x T x C -> B x C x T\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for f in self.convolutions:\n",
    "            # Case for upsampling\n",
    "            if speaker_embed_btc is not None and speaker_embed_btc.size(1) != x.size(-1):\n",
    "                speaker_embed_btc = expand_speaker_embed(x, speaker_embed, tdim=-1)\n",
    "                speaker_embed_btc = F.dropout(\n",
    "                    speaker_embed_btc, p=self.dropout, training=self.training)\n",
    "            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n",
    "\n",
    "        # Back to B x T x C\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
